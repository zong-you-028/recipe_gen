import os
import json
from huggingface_hub import hf_hub_download

# === è‡ªå®šå„²å­˜ä½ç½® ===
local_dir = "./LLaVA-Chef-Local"
os.makedirs(local_dir, exist_ok=True)

# === å¿…è¦æª”æ¡ˆ ===
files_to_download = [
    "tokenizer.model",              # LLaMA tokenizer çš„æ ¸å¿ƒ
    "tokenizer_config.json",        # åŒ…å« pad_token è¨­å®š
    "special_tokens_map.json",      # åŒ…å« <image> ç­‰ç‰¹æ®Šç¬¦è™Ÿ
]

# === ä¸‹è¼‰æ‰€æœ‰ tokenizer ç›¸é—œæª”æ¡ˆ ===
for fname in files_to_download:
    print(f"ğŸ“¥ Downloading {fname} ...")
    hf_hub_download(
        repo_id="mohbattharani/LLaVA-Chef",
        filename=fname,
        local_dir=local_dir,
        local_dir_use_symlinks=False,
    )

# === ä¿®æ­£ pad_token ===
tokenizer_config_path = os.path.join(local_dir, "tokenizer_config.json")
print("ğŸ”§ Fixing tokenizer_config.json ...")

with open(tokenizer_config_path, "r", encoding="utf-8") as f:
    config = json.load(f)

if isinstance(config.get("pad_token"), int):
    config["pad_token"] = "[PAD]"
    with open(tokenizer_config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)
    print("âœ… ä¿®æ­£å®Œæˆ pad_token ç‚º '[PAD]'")
else:
    print("âœ… ä¸éœ€ä¿®æ­£ pad_token")

# === æç¤º ===
print("\nğŸ“‚ è«‹åœ¨ä¸»ç¨‹å¼ä¸­è¨­å®šä»¥ä¸‹è·¯å¾‘ï¼š")
print(f"model_path = r\"{local_dir}\"")
